# M1.6 Performance Benchmarking Infrastructure

## Overview

M1.6 introduces proper performance benchmarking infrastructure using BenchmarkDotNet to establish reliable M1.5 baseline measurements. This addresses critical methodology issues discovered in previous performance testing.

## Motivation

Previous performance testing used single-execution measurements with `System.Diagnostics.Stopwatch`, which suffered from:

- **No JIT warmup**: Cold compilation affects first-execution timing
- **Single measurements**: No statistical averaging or confidence intervals  
- **GC interference**: Garbage collection timing artifacts
- **No outlier detection**: Results vulnerable to system noise

## BenchmarkDotNet Implementation

### Features

- **Automatic JIT warmup**: 3 warmup iterations ensure compiled code measurement
- **Statistical rigor**: 10 measurement iterations with mean, error, standard deviation
- **Memory diagnostics**: Accurate memory allocation measurement
- **Outlier detection**: Automatic filtering of measurement artifacts
- **Comparative analysis**: Baseline ratios and ranking

### Benchmark Categories

1. **Scale Benchmarks** - Performance vs. model size
   - Small Scale: 10 nodes, 100 bins
   - Medium Scale: 100 nodes, 1000 bins  
   - Large Scale: 1000 nodes, 1000 bins

2. **Expression Type Benchmarks** - Performance by expression complexity
   - Simple: `base_X * 1.5`
   - Complex: `MIN(base_X * 2, base_Y)`
   - SHIFT: `base_X + SHIFT(base_Y, 1)`

3. **End-to-End Benchmarks** - Complete parse + evaluate workflow

### Running Benchmarks

#### Development Testing
```bash
# Run quick benchmark subset for development
dotnet test --filter "FullyQualifiedName~M15BenchmarkRunner.RunM15ScaleBenchmarks"
```

#### Full Analysis
```bash
# Run comprehensive benchmark suite (takes 10+ minutes)
dotnet test --filter "FullyQualifiedName~M15BenchmarkRunner.RunAllM15Benchmarks"
```

#### Specific Categories
```bash
# Expression type comparison
dotnet test --filter "FullyQualifiedName~M15BenchmarkRunner.RunM15ExpressionTypeBenchmarks"

# End-to-end performance
dotnet test --filter "FullyQualifiedName~M15BenchmarkRunner.RunM15EndToEndBenchmarks"
```

## M1.5 Baseline Establishment

### Goals

1. **Establish reliable M1.5 baselines** for future comparison
2. **Validate performance characteristics** of Expression Language implementation
3. **Identify scaling bottlenecks** across different workload sizes
4. **Create measurement methodology** for M2 PMF comparison

### Expected Results

Based on preliminary testing (with proper methodology), M1.5 baseline should show:

- **Parse Performance**: Linear scaling with node count
- **Evaluation Performance**: Linear scaling with bins Ã— nodes
- **Memory Usage**: Proportional to model complexity
- **Expression Overhead**: Minimal impact of expression complexity

## Integration with M2

Once M1.6 establishes reliable baselines, this infrastructure will be merged into M2 to enable:

1. **Accurate M2 vs M1.5 comparison** using identical methodology
2. **PMF performance characterization** with statistical confidence
3. **Regression detection** through automated benchmarking
4. **Production readiness assessment** based on reliable metrics

## Files

- `M15BenchmarkDotNetTests.cs` - BenchmarkDotNet performance tests
- `M15BenchmarkRunner.cs` - xUnit test runners for different benchmark categories
- This document - Usage and methodology guide

## Next Steps

1. Run M1.6 benchmarks to establish M1.5 baselines
2. Document performance characteristics and scaling behavior
3. Merge benchmarking infrastructure into M2
4. Re-measure M2 PMF performance with proper methodology
5. Compare M2 vs M1.5 with statistical confidence
