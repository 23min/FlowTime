# M-03.02.01 ‚Äî Simulation Run Orchestration

**Status:** üöß In Progress  
**Dependencies:** M-03.00 (Foundation + Fixtures), SIM-M-03.00 (Schema Foundations & Validation), M-03.01 (Time-Travel State APIs), M-03.02 (Telemetry Capture & Templates)

**Target:** Enable `/v1/runs` to generate gold bundles using **simulation mode** without requiring telemetry capture input. The milestone introduces a simulation-only orchestration pipeline, returning canonical artifacts (`run.json`, `manifest.json`, `series/index.json`) and structured metadata so downstream APIs and the UI can consume newly generated runs immediately.

---

## Overview

M-03.00 through M-03.02 laid the groundwork for time-travel: canonical run artifacts, mode-aware validation, and telemetry capture/bundling. However, `/v1/runs` currently assumes telemetry-backed runs. Simulation mode‚Äîwhere FlowTime.Sim generates synthetic telemetry directly from templates‚Äîremains CLI-only. UI milestones (UI-M-03.12) and operator workflows now depend on a REST surface that can produce a gold run without telemetry inputs.

M-03.02.01 bridges that gap. It delivers simulation-mode orchestration for `/v1/runs` and the CLI, producing canonical outputs with warning/error semantics aligned to simulation rules. Telemetry mode continues to function unchanged; this milestone focuses on the simulation path so validation/polish work in M-03.03 can apply uniformly across both modes.

---

## Scope

### In Scope ‚úÖ
1. Extend `RunOrchestrationService` (and related APIs) to support simulation mode without requiring telemetry bindings.
2. Emit canonical run artifacts (`run.json`, `manifest.json`, `series/index.json`) using FlowTime.Sim templates and generated series data.
3. Return structured metadata/warnings in the API response mirroring `/state` contracts.
4. Update CLI (`flowtime run`) to reuse the shared orchestration for simulation runs.
5. Observability: structured logs and metrics for simulation runs.
6. Documentation updates (milestone, API walkthroughs, operator guide).

### Out of Scope ‚ùå
- Telemetry capture or loader improvements (already handled in M-03.02 / future telemetry milestones).
- Post-run validation polish covered by M-03.03.
- Advanced orchestration features (cancellation, multi-tenancy, ADX ingestion).

### Future Work
- Validation polish and warning surfacing (M-03.03).
- Unified orchestration UX (UI-M-03.12+) once simulation mode is available via API.

---

## Requirements

### Functional Requirements

#### FR1: Simulation Run Creation
- `POST /v1/runs` with `mode=simulation` generates a gold bundle from the specified template and parameters.
- Accepts optional deterministic run id and parameter overrides (JSON object).
- Returns metadata: `runId`, `mode`, `templateId`, warning count, grid summary, artifact presence flags, `canReplay`.
- Validates required template configuration (window/topology); returns `400` on validation failure.

#### FR2: Artifact Emission
- Writes canonical files under `data/runs/<runId>/`:
  - `run.json` with grid, warnings, provenance (mode=simulation).
  - `manifest.json` summarising hashes/series.
  - `series/index.json` listing series ids and sample counts.
- Ensures idempotent behaviour when `deterministicRunId=true`.

#### FR3: Logging & Metrics
- Emit structured logs for orchestration stages (start, validation, evaluation, artifact write, completion/failure).
- Produce metrics counters/histograms mirroring telemetry mode (`run_created_total`, `simulation_evaluation_duration_ms`, etc.).

#### FR4: CLI Reuse
- Update CLI (`flowtime run ... --mode simulation`) to call the shared service instead of bespoke logic.
- Provide parity integration tests to guard against drift.

### Non-Functional Requirements
- **NFR1 (Performance):** Simulation orchestration should complete within 2√ó the current CLI baseline for the order-system template.
- **NFR2 (Observability):** Logs include `runId`, `templateId`, `mode`; metrics available for dashboards.
- **NFR3 (Validation Enforcement):** Simulation mode fails fast on validation errors (no silent warning downgrade).

---

## Current Progress (2025-10-19)

- ‚úÖ `RunOrchestrationService` branches on simulation mode, generates FlowTime.Sim series, writes canonical artifacts, emits telemetry manifests, and produces structured logs/metrics (`run_created_total`, `simulation_evaluation_duration_ms`).
- ‚úÖ `/v1/runs` accepts simulation payloads, returns `/state`-aligned metadata, enforces window/topology validation, and surfaces template errors as `400` responses.
- ‚úÖ CLI `flowtime run --template-id <id> --mode simulation` reuses the shared orchestration; telemetry path remains available for capture-backed runs.
- ‚úÖ Integration, golden, and CLI tests cover simulation orchestration; `FlowTime.API.http` now documents simulation samples alongside telemetry flows.
- ‚úÖ Observability automated: unit tests assert metric counters/histogram emission and `RunOrchestrationCompleted` logging for simulation runs.

## Implementation Plan

### Phase 1: Service Layer
- Refactor `RunOrchestrationService` to branch on mode. For simulation:
  - Generate synthetic data via FlowTime.Sim services.
  - Construct canonical artifacts (reuse manifest/index writers).
  - Apply simulation-specific validation/warning semantics.
- Introduce reusable DTOs (request/response) to keep API & CLI aligned.
- Follow strict TDD (Red ‚Üí Green ‚Üí Refactor) for all changes: write failing tests first before implementing service logic.

### Phase 2: API & CLI Integration
- Update `POST /v1/runs` handler to call the refactored service for simulation mode.
- Ensure response payloads align with existing telemetry metadata envelope.
- Update CLI (`flowtime run`) and tests to leverage the new service; maintain telemetry behaviour.
- Maintain TDD discipline‚Äîadd/extend tests that fail before changing API/CLI code, then refactor once green.

### Phase 3: Tests, Observability, Docs
- Add unit tests for simulation request validation, orchestration, and artifact generation.
- Integration tests: create simulation run, inspect artifacts, verify `/state` compatibility.
- Capture golden response samples for API regression tests.
- Update docs: milestone spec, tracking, API `.http` examples, operator guides.
- Confirm structured logging/metrics for simulation runs.
- Continue Red‚ÄìGreen‚ÄìRefactor cycle; expand tests before implementation adjustments.

---

## Test Plan

### Unit Tests
- `SimulationRunRequestValidatorTests`
- `SimulationOrchestrationServiceTests`
- `SimulationArtifactWriterTests`

### Integration Tests
- `RunApiTests.CreateSimulationRun_WritesArtifacts`
- `RunApiTests.CreateSimulationRun_ValidationFailure`
- `RunApiTests.SimulationResponse_MetadataMatchesState`
- CLI end-to-end test invoking simulation mode and verifying output.

### Golden/Regression Tests
- JSON snapshots for `POST /v1/runs` (simulation response) and resulting `run.json` sample.

### Observability Verification
- Capture logs/metrics via in-memory providers, asserting required fields.

---

## Deliverables
- Updated orchestration service and API endpoints supporting simulation mode.
- Canonical artifact writers invoked for simulation runs.
- CLI reuse of orchestration service.
- Automated tests (unit, integration, golden) added to `dotnet test FlowTime.sln` pipeline.
- Documentation updates (`docs/milestones`, operator guides, `.http` samples).
- Release notes summarising simulation-mode support for `/v1/runs`.

---

## Success Criteria
- `POST /v1/runs` simulation requests succeed without telemetry inputs and produce canonical artifacts.
- API responses include metadata enabling `/state` consumption and UI integration.
- Tests pass across CI, covering simulation paths.
- Observability instrumentation (logs/metrics) is in place for simulation runs.
- Documentation and examples guide operators/UI teams on using simulation mode.
