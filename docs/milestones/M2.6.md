# Milestone M2.6 — Full Loop Modeling: Export/Import Telemetry Cycle

## Goal

Close the fundamental model### **Essential Commands:**
```bash
# Artifacts management
flowtime list --kind run --tag baseline --last 7d    # List artifacts with filters
flowtime open --id run_2025-09-19T10:22Z_c2c6        # Open artifact in UI
flowtime run --model <model_id> --out artifacts      # Run model artifact, creates Run artifact

# Export operations (creates files, not artifacts)
flowtime export --run <runId> --format gold --out data.csv
flowtime export --run <runId> --format ndjson --out data.ndjson
flowtime export --run <runId> --format parquet --out data.parquet

# Import operations (creates Telemetry artifacts)
flowtime import --data external.csv --out artifacts       # Creates telemetry artifact
flowtime import --data external.ndjson --out artifacts
flowtime import --data external.parquet --out artifacts

# Validate complete loop works
flowtime test-loop --model <model_id>  # run → export → import → verify
```el → Compute → Export → Import** with persistent artifact storage. This milestone establishes the export/import infrastructure and persistent artifacts registry to enable durable data flow cycles. The system maintains a complete catalog of all models, runs, and imported telemetry so the UI never "forgets" previous work. The focus is on completing the data flow cycle with full persistence rather than analysis features.

## Context & Vision

Closing the modeling loop with persistence enables the core FlowTime value proposition:

1. **Model Phase**: Create models with optional PMF-based uncertainty (stored as Model artifacts)
2. **Compute Phase**: Run models through FlowTime Engine (stored as Run artifacts)
3. **Export Phase**: Export results as standardized telemetry formats
4. **Import Phase**: Import telemetry data, reconstructing the original DAG (stored as Telemetry artifacts)
5. **Registry Phase**: Maintain persistent catalog of all artifacts for discoverability

This establishes the **foundational data flow with persistence** that enables all future iterative modeling capabilities. The emphasis is on making the loop work with full artifact durability so users never lose their work.

## Why M2.6 (Interposed Milestone)

This milestone is strategically positioned between M2 (PMF Support) and M3 (Backlog/Latency) because:

- **Foundation First**: Export/import infrastructure must exist before advanced state modeling
- **Architecture Decision**: Telemetry formats influence all future feature design
- **Data Flow Completion**: Establishes the core loop that enables all future analysis capabilities
- **Preparation for Analysis**: Creates the foundation for comparison and iterative modeling in future milestones

## Functional Requirements

### **FR-M2.6-1: Export System (Close the Loop)**
FlowTime Engine can export run results to standardized formats for analysis and data interchange.

**Core Export Capability:**
- **Gold CSV**: Standard telemetry format `time_bin,component_id,measure,value`
- **Series NDJSON**: Newline-delimited JSON for streaming and programmatic access
- **Parquet**: Columnar format for efficient storage and analytics integration
- **Complete Run Export**: All series + metadata in single operation

**Loop Closure:**
- Export maintains artifact determinism (reproducible hashes)
- Compatible with external analysis tools (Excel, R, Python)
- Foundation for importing results back into FlowTime systems

### **FR-M2.6-2: Persistent Artifacts Registry** (New!)
FlowTime maintains a persistent catalog of all models, runs, and imported telemetry with full metadata.

**Artifact Types:**
- **Model Artifacts**: From flowtime-sim (model.yaml + catalog.json + optional preview.svg)
- **Run Artifacts**: From flowtime engine execution (binned_v0.csv + catalog.json)
- **Telemetry Artifacts**: From external data import (binned_v0.csv + catalog.json)

**Storage Architecture:**
```
/artifacts/
  models/{model_id}/v{n}/          # Model artifacts with versioning
    model.yaml
    catalog.json
    preview.svg                    # Optional DAG preview
  runs/{run_id}/                   # Run artifacts
    binned_v0.csv
    catalog.json
  telemetry/{telemetry_id}/        # Imported telemetry
    binned_v0.csv
    catalog.json
```

**Catalog Schema:**
```json
{
  "kind": "model|run|telemetry",
  "id": "run_2025-09-19T10:22Z_c2c6", 
  "name": "checkout-baseline",
  "created_utc": "2025-09-19T10:22:30Z",
  "schema_version": "treehouse.binned.v0",
  "model_id": "model_checkout_v1",
  "engine_version": "ft-eng 0.2.1",
  "topology": {"nodes": [...], "edges": [...]},
  "capabilities": ["counts","latency.quantiles"],
  "tags": ["orders","baseline","weekly"],
  "source": "sim|engine|import",
  "inputs_hash": "sha256:...",
  "owner": "user@domain.com",
  "visibility": "private|team|public"
}
```

### **FR-M2.6-3: Import & DAG Reconstruction** 
FlowTime Engine can import exported data and reconstruct the original computational DAG.

**Import Capability:**
- **Gold CSV Import**: Read standard telemetry format back into FlowTime
- **NDJSON Import**: Import newline-delimited JSON format  
- **Parquet Import**: Import columnar format data
- **Bundle Import**: Import complete model+run packages
- **Data Validation**: Ensure imported data matches expected schema and grid alignment
- **DAG Reconstruction**: Restore the same computational graph structure as the original run
- **Artifact Creation**: Create Telemetry artifacts for imported data with catalog.json

**Loop Closure:**
- **Round-trip Integrity**: Export → import is 100% deterministic with identical data
- **Metadata Preservation**: PMF definitions (when present), node relationships, and timing information preserved
- **Persistent Storage**: All imported data becomes discoverable artifacts in the registry

### **FR-M2.6-3: Enhanced CLI Operations**
CLI supports artifacts registry management and export/import operations for closing the modeling loop.

**Artifacts Registry Commands:**
```bash
# List and search artifacts
flowtime list --kind run|model|telemetry --tag baseline --last 7d
flowtime search --query checkout --owner user@domain.com

# Work with artifacts directly
flowtime run --model <model_id> --out artifacts       # Run model artifact, creates Run artifact
flowtime open --id run_2025-09-19T10:22Z_c2c6        # Open artifact in UI
flowtime delete --id <artifact_id> --confirm         # Delete with confirmation

# Compare artifacts (future milestone preparation)
flowtime compare --baseline <run_id> --comparison <run_id2>
```

**Export/Import Commands:**
```bash
# Export run for external analysis or import elsewhere
flowtime export --run <runId> --format gold --out data.csv
flowtime export --run <runId> --format ndjson --out data.ndjson
flowtime export --run <runId> --format parquet --out data.parquet

# Import external data back into FlowTime (creates Telemetry artifacts)
flowtime import --data external.csv --out artifacts --name "prod-baseline"
flowtime import --data external.ndjson --out artifacts --tags "production,q3"
flowtime import --data external.parquet --out artifacts --validate-schema

# Validate complete loop works
flowtime test-loop --model <model_id>  # run → export → import → verify
```

**Loop Validation:**
- Commands work together to demonstrate complete data flow
- Export/import round-trip maintains data integrity and DAG structure
- Imported runs ready for future analysis capabilities

### **FR-M2.6-4: FlowTime API Artifacts Endpoints** (Enhanced!)
Service endpoints support artifacts registry, export, and import operations.

**Artifacts Registry API:**
```http
GET  /v1/artifacts                       # List/search artifacts with filters
GET  /v1/artifacts/{id}                  # Get artifact catalog + file links
POST /v1/artifacts/index                 # Rebuild index from disk
DELETE /v1/artifacts/{id}                # Delete artifact with confirmation
```

**Enhanced Export/Import API:**
```http
GET  /v1/runs/{runId}/export/{format}    # Export run in Gold CSV/NDJSON/Parquet format
POST /v1/runs                            # Run an artifact (model_id or telemetry_id)
POST /v1/import                          # Import data and create Telemetry artifact
```

**Query Parameters:**
```http
GET /v1/artifacts?kind=run|model|telemetry&query=checkout&tag=baseline&from=2025-09-01&to=2025-09-30&page=1&size=20&sort=created_desc
```

**Response Format:**
```json
{
  "artifacts": [{
    "id": "run_2025-09-19T10:22Z_c2c6",
    "kind": "run", 
    "name": "checkout-baseline",
    "created_utc": "2025-09-19T10:22:30Z",
    "tags": ["orders","baseline"],
    "owner": "user@domain.com",
    "files": {
      "catalog": "/artifacts/runs/.../catalog.json",
      "data": "/artifacts/runs/.../binned_v0.csv"
    }
  }],
  "total": 156,
  "page": 1
}
```

## Technical Architecture

### **Export Pipeline**
```
Run Artifacts → Format Converter → Export Writers → External Tools
     ↓              ↓                    ↓               ↓
  Series Data   Gold CSV Format    File System     Excel/Tableau
  PMF Data      NDJSON Streaming   HTTP Streaming  Python/R
  Metadata      Parquet Columnar   Artifact Store  Analytics Tools
```

### **Persistent Artifacts Architecture** (New!)
```
Artifact Creation → Storage Layer → Registry Index → API Layer → UI Discovery
        ↓               ↓              ↓             ↓            ↓
   Model/Run/       File System    catalog.json   REST API    Searchable
   Telemetry        /artifacts/    Index Scan     Endpoints   Cards/Lists
   Generation       Layout         Query Engine   Filtering   Deep Links
```

### **Import/Reconstruction Architecture** 
```
Import Request → FlowTime API → Import Engine → DAG Reconstruction → Artifact Creation → Registry Update
       ↓              ↓              ↓               ↓                      ↓                ↓
  External Data   Validate Schema   Parse Metadata   Rebuild Graph     Create Telemetry   Update Index
  CSV/NDJSON/     Data Integrity    PMF Definitions  Node Relationships  Artifact        Add to Registry
  Parquet Files   Grid Alignment    Timing Info      Series Data         catalog.json    
```

## Example Workflows

### **Basic Loop Closure with Artifacts Registry**
```yaml
# 1. Initial baseline model (will be saved as Model artifact)
nodes:
  - id: user_requests
    kind: pmf
    pmf: { "100": 0.3, "150": 0.5, "200": 0.2 }
  - id: auth_service
    kind: expr
    expr: "user_requests * 0.95"
```

```bash
# 2. Create and run model (creates Model and Run artifacts)
flowtime sim new --template checkout --name baseline-v1 --out artifacts
# Creates: /artifacts/models/baseline-v1_c2c6/v1/{model.yaml, catalog.json}

flowtime run --model baseline-v1_c2c6 --out artifacts --name "baseline-run"
# Creates: /artifacts/runs/baseline-run_2025-09-19_d4f7/{binned_v0.csv, catalog.json}

# 3. Export for external analysis (file operations, not artifacts)
flowtime export --run baseline-run_2025-09-19_d4f7 --format gold --out exports/baseline.csv
flowtime export --run baseline-run_2025-09-19_d4f7 --format parquet --out exports/baseline.parquet

# 4. Import data back creating Telemetry artifact
flowtime import --data exports/baseline.csv --out artifacts --name "baseline-reimport" --tags "validation,loop-test"
# Creates: /artifacts/telemetry/telem_2025-09-19_e8a1/{binned_v0.csv, catalog.json}

# 5. List all created artifacts
flowtime list --kind all --last 1h
# Shows: baseline-v1 (model), baseline-run (run), baseline-reimport (telemetry)

# 6. Verify loop integrity with artifact IDs
flowtime test-loop --model baseline-v1_c2c6  # Complete cycle validation
```

### **Multi-Session Workflow (Persistence)**
```bash
# Session 1: Create and run models (all become artifacts)
flowtime sim new --template e-commerce --name shop-conservative --out artifacts
flowtime sim new --template e-commerce --name shop-optimistic --out artifacts
flowtime run --model shop-conservative_a1b2 --out artifacts --tags "baseline,conservative"
flowtime run --model shop-optimistic_c3d4 --out artifacts --tags "scenario,optimistic"

# Session 1 ends, user closes browser/app

# Session 2: Resume work (artifacts persist)
flowtime list --kind run --tag baseline --last 7d
# Shows: shop-conservative run from previous session

flowtime list --kind model --query shop
# Shows: both shop-conservative and shop-optimistic models

# Export for external analysis using artifact IDs
flowtime export --run shop-conservative_run_2025-09-19_x7y8 --format parquet --out analysis/conservative.parquet

# Import production data for comparison preparation
flowtime import --data prod-sept-data.csv --out artifacts --name "prod-sept-baseline" --tags "production,september"

# List all artifacts for this project
flowtime list --tag baseline,scenario,production --sort created_desc
```

## Implementation Phases

### **Phase 1: Export Foundation**
- Implement Gold CSV, NDJSON, and Parquet export in FlowTime.Core
- Export API endpoints: GET /v1/runs/{id}/export/{format}
- CLI export command with format validation
- Maintain artifact determinism and content hashing

### **Phase 2: Import & DAG Reconstruction**
- Import capability for all export formats (CSV, NDJSON, Parquet)
- Schema validation and grid alignment for imported data
- DAG reconstruction from imported metadata
- CLI import command with validation feedback

### **Phase 3: Loop Validation**
- Round-trip testing: export → import → verify integrity
- CLI test-loop command for complete cycle validation
- Integration testing: run → export → import → reconstruct
- Format-specific validation for each export type

### **Phase 4: Polish & Documentation**
- Error handling and user feedback improvements
- UI integration for export/import operations
- Documentation: complete loop workflow guide
- Format specification documentation

## New Code/Files

```
src/FlowTime.Core/Artifacts/        # NEW: Artifacts registry system
  ArtifactsRegistry.cs               # Main registry orchestration
  ArtifactCatalog.cs                 # catalog.json schema and operations
  ArtifactStorage.cs                 # File system storage abstraction
  ArtifactIndex.cs                   # Search and query engine
  ArtifactTypes.cs                   # Model/Run/Telemetry artifact definitions
  
src/FlowTime.Core/Export/           # Export functionality
  GoldCsvExporter.cs                 # Gold CSV format export
  NdjsonExporter.cs                  # NDJSON (newline-delimited JSON) export
  ParquetExporter.cs                 # Parquet columnar format export
  RunMetadataExporter.cs             # Metadata and PMF definitions
  
src/FlowTime.Core/Import/           # Import functionality
  GoldCsvImporter.cs                 # Gold CSV format import
  NdjsonImporter.cs                  # NDJSON format import
  ParquetImporter.cs                 # Parquet format import
  DagReconstructor.cs                # Rebuild computational graph from metadata
  SchemaValidator.cs                 # Validate imported data format across all formats
  ArtifactCreator.cs                 # NEW: Create artifacts from imported data
  
src/FlowTime.API/Endpoints/         # API endpoints
  ArtifactsEndpoints.cs              # NEW: Artifacts registry endpoints
  ExportEndpoints.cs                 # Export format endpoints
  ImportEndpoints.cs                 # Import endpoints
  
src/FlowTime.Cli/Commands/          # CLI support
  ExportCommand.cs                   # flowtime export command
  ImportCommand.cs                   # NEW: flowtime import command
  TestLoopCommand.cs                 # NEW: flowtime test-loop command
  ListCommand.cs                     # NEW: flowtime list command (artifacts registry)
  SearchCommand.cs                   # NEW: flowtime search command (artifacts registry)
  OpenCommand.cs                     # NEW: flowtime open command (open artifact in UI)
  DeleteCommand.cs                   # NEW: flowtime delete command (remove artifact)
  
ui/FlowTime.UI/Components/Export/   # NEW: UI components for loop closure
  ExportRunDialog.razor              # Export functionality
  ImportDataDialog.razor             # Import functionality
  LoopStatusIndicator.razor          # Show loop closure status
  
tests/FlowTime.Tests/Export/        # Export testing
  ExportEngineTests.cs               # Core export functionality
  ApiExportTests.cs                  # API endpoint testing
  
tests/FlowTime.Tests/Import/        # NEW: Import testing
  ImportEngineTests.cs               # Core import functionality
  DagReconstructionTests.cs          # DAG rebuild testing
  RoundTripTests.cs                  # Export → import integrity

tests/FlowTime.Tests/Artifacts/     # NEW: Artifacts registry testing
  ArtifactsRegistryTests.cs          # Core registry functionality
  ArtifactCatalogTests.cs            # Catalog schema and validation
  ArtifactStorageTests.cs            # Storage layer testing
  ArtifactIndexTests.cs              # Search and query testing
  ApiArtifactsEndpointsTests.cs      # API endpoint testing
  
docs/guides/
  export-import-loop.md              # Complete loop workflow documentation
  data-formats.md                    # Gold CSV, NDJSON, and Parquet format specifications
```

## Acceptance Criteria

### **Artifacts Registry**
- ✅ All models, runs, and imported telemetry persist as discoverable artifacts
- ✅ Artifacts registry provides search, filtering, and tag-based organization
- ✅ Catalog metadata includes topology, capabilities, tags, and provenance
- ✅ Storage layout supports local filesystem, S3, and ADLS backends
- ✅ Registry index rebuilds automatically from disk on cold start
- ✅ Deep links enable direct artifact access via URL

### **Export Capabilities**
- ✅ Run artifacts export to Gold CSV format with standard schema
- ✅ NDJSON export enables streaming and programmatic access
- ✅ Parquet export provides efficient columnar storage for analytics
- ✅ Export CLI command produces predictable directory structure
- ✅ Export API endpoint supports all formats efficiently
- ✅ Exported data maintains artifact determinism and content hashing

### **Import Capabilities**
- ✅ All export formats (CSV, NDJSON, Parquet) import correctly
- ✅ Import creates persistent Telemetry artifacts with full catalog metadata
- ✅ DAG reconstruction creates identical computational graph as original run
- ✅ Schema validation catches format mismatches with clear error messages

### **Loop Closure**
- ✅ Round-trip integrity: export → import is 100% deterministic with identical data
- ✅ CLI `test-loop` command validates complete cycle automatically
- ✅ Imported runs become persistent artifacts ready for future analysis
- ✅ External tool integration (Excel, Python, R, BI tools) works with exported data



## Dependencies & Prerequisites

### **Completed Milestones**
- **M2** (PMF Support): PMF nodes provide uncertainty modeling foundation
- **SVC-M1** (Artifact Serving): API artifact endpoints provide run data access foundation
- **UI-M2** (Mode Toggle): UI can interface with both FlowTime-Sim and FlowTime Engine

### **Parallel Development**
- **FlowTime-Sim M3**: Artifact compatibility and export format alignment
- **UI-M3** (future): Export/import UI integration for loop closure workflows

### **External Dependencies**
- No new external packages required
- CSV export/import: existing System.Text.Json and built-in file I/O
- DAG reconstruction: leverage existing FlowTime.Core graph structures

## Success Metrics

### **Artifacts Registry Metrics**
- **Persistence**: 100% of models, runs, and imports become discoverable artifacts
- **Session Continuity**: Users can resume work across browser/app restarts
- **Search Performance**: Artifact queries return results within 500ms for typical catalogs (<1000 items)
- **Storage Efficiency**: Catalog.json files remain under 10KB for typical models
- **Index Rebuild**: Cold start index rebuild completes within 30 seconds for 500 artifacts

### **Loop Closure Metrics**
- **Round-trip Integrity**: Export → Import is 100% deterministic with identical data
- **Format Compatibility**: All formats (CSV, NDJSON, Parquet) validate against schemas
- **Schema Validation**: Import correctly identifies and handles format mismatches
- **End-to-End Testing**: `flowtime test-loop` successfully validates complete cycle
- **DAG Reconstruction**: Imported runs have identical structure to original runs

### **Integration Metrics**
- **CLI Workflow**: Commands work together without requiring documentation
- **External Tools**: Exported data opens correctly in Excel, R, Python pandas, BI tools
- **Data Quality**: No precision loss in numeric values during export/import cycle
- **Format Coverage**: All three formats work seamlessly with their target use cases
- **Artifact Discoverability**: Users find previously created artifacts within 3 CLI commands or UI clicks
- **Error Handling**: Clear error messages guide users when schema/format issues occur

## Questions for Review

1. **Scope Appropriateness**: Does this simplified M2.6 represent the right foundation for loop closure?

2. **DAG Reconstruction**: Is rebuilding the computational graph from imported data the right approach?

3. **Export Format Coverage**: Do Gold CSV, NDJSON, and Parquet formats meet external tool integration needs?

4. **Future Preparation**: Does this foundation properly prepare for comparison capabilities in future milestones?

5. **CLI Workflow**: Does the proposed `test-loop` command provide sufficient validation of the complete cycle?

## Next Steps

1. **Export System Design**: Finalize Gold CSV, NDJSON, and Parquet format specifications
2. **Import Architecture**: Design DAG reconstruction algorithm from exported metadata
3. **API Contract Definition**: Design export/import endpoint request/response formats
4. **UI Integration**: Plan minimal UI components for export/import operations
5. **Validation Framework**: Define round-trip integrity tests and schema validation rules

---

## Revision History

| Date | Change | Author |
|------|--------|--------|
| 2025-09-18 | Initial M2.6 milestone specification created (renamed from M2.5) | Assistant |